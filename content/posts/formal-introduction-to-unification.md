+++
title = 'Formal Introduction to Unification'
date = 2024-03-16T16:05:13-10:00
draft = false
+++

$$
\def\por{\ \ | \ \ }
\def\dom{\textsf{dom}}
\def\tsf#1{\textsf{#1}}
\def\unifyj#1#2#3{#1 \sim #2 \Downarrow #3}
\def\dict#1#2{\textsf{Dict}\left( #1, #2 \right)}
$$



Unification is probably one of the most important concepts in computer science. If you are not aware, unification is basically the process you undergo to find out how two syntactic objects like $a+b$ and $a+2c$ can be the same thing. In this case, you replace $b$ with $2c$. Note that you don't replace $2c$ with $b$, as $2\cdot c$ is less general than $b$, since $2\cdot c$ contains the literal $2$. 

Daniel created a language called Bergamot (https://danilafe.com/blog/bergamot/) that allows you to write expressions and unify them, while being able to see a trace of the unification process. Prolog is a very popular language that is based on unification, and belongs to the class of Logic Programming Languages. There are several projects that implement unification algorithms and give some sort of trace of the process. Though, Daniel's Bergamot is distinguished among these, as Bergamot produces a specific kind of trace called a *proof tree*. 

Proof trees can come in different forms. The kind of proof trees that Bergamot outputs are widely used in Programming Language Theory, and are the kind that I am interested in. They are generated by recursive application of inference rules (see Bergamot post) of the form
$$
\dfrac{P_1\quad \ldots \quad P_n}{Q}\ \texttt{RuleName}
$$

Show basic example of proof tree application. use < example. 

Proof trees are great explanations. But how do you make them? By hand, it is usually not so hard, since humans are generally pretty good at unification. But how does a computer do it? More precisely, how do you make a program that looks at a bunch of rules, along with a conclusion, and produces a proof tree? 

While the unification process has been formalized in many places, the process of proof tree construction has not. I really want to formally describe the proof tree generation process in its entirety, but in order for it to make any sense, I need to tell you about how unification works. Here I will try to give a formal(ish) intro to unification. Then later, I will tell you about how you can make proof trees. 

### Unification



Two terms $t_1$ and $t_2$ are unifiable if we can replace shared variables of $t_1$ and $t_2$ with certain terms, resulting in two equal terms. This replacement information is encoded in an object called a substitution, which is denoted by $\sigma$ (and sometimes $\tau$). You can think of substitutions as a dictionary that maps variables to terms. The result of replacing $t$'s variables with their coresponding terms in $\sigma$, is denoted by $t[\sigma]$. Then, if $t_1[\sigma]=t_2[\sigma]$ is the case, then $\sigma$ is called a unifier for $t_1$ and $t_2$. In other words, $\sigma$ unifies $t_1$ and $t_2$​​. Now, to improve the initial definition, two terms $t_1$ and $t_2$​​ are said to be unifiable if they have a unifier. 



The formalization starts with the language of terms for which we are unifying. Terms can be one of two things, (i) a variable $x\in \textsf{Var}$, or (ii) a construct $f(t_1,\ldots,t_n)$ of zero or more terms $t_1,\ldots,t_n\in\tsf{Term}$, with the constructor $f\in \tsf{Name}$. The $\tsf{Term}$ language is defined by the grammar
$$
%\begin{align*}
%t\in\tsf{Term} ::= & \  x \por f(t^n)\\
%t^n \eqdef & \begin{cases} & \text{if $n = 0$}\\ t_1,\ldots,t_n & \text{otherwise}\end{cases}
%\end{align*}
t\in\tsf{Term} ::= x \por f(t_1,\ldots,t_n)
$$
where $x\in \tsf{Var}$, $f\in \tsf{Name}$, and $n\in\N$. It may seem odd that constructors are just names, but this is what allows us to represent an expression from any language as a term. 

Consider our $a+b$ and $a+2c$ example from above. We can write $a+b$ as $+(a,b)$, where $a,b\in \tsf{Var}$ and $+\in \tsf{Name}$. Then for $a+2c=a+2\times c$, we have $+(a,\times(2(),c))$, where $a,c\in \tsf{Var}$ and the constructors $+,\times,2\in \tsf{Name}$​. For the sake of aesthetics, constructed terms with zero arguments will be written without parenthesis. That is, if $f\in \tsf{Name}$, then $f()\in\tsf{Term}$ is written as $f\in\tsf{Term}$. In summary,
$$
\begin{align*} a+b & \to +(a,b)\\\\[8pt]
a+2c=a+2\times c & \to +(a,\times(2(),c))=+(a,\times(2,c))
\end{align*}
$$
where $a,b,c\in \tsf{Var}$ and $+,\times,2\in \tsf{Name}$.

When a variable $x\in\tsf{Var}$ is used in a term $t\in \tsf{Term}$, it represents an abstraction over terms that can exist in place of $x$. Another view that I like, is that terms which have one or more variables act as templates for all other terms that may be unified. The set of variables of a term $t$ is called the *free variables* of $t$, and is denoted by $\freevars(t)$. The function $\freevars$ from terms to sets of variables can easily be defined,
$$
\freevars : \tsf{Term}\to \mathcal{P}\left(\tsf{Var}\right) \qquad\qquad \freevars(t)\eqdef\begin{cases}\lbrace x \rbrace & \text{if $t=x\in\tsf{Var}$}\\\ \bigcup\limits_{i=1}^n \freevars(t_i) & \text{if $t=f(t_1,\ldots,t_n)$}\\\
\varnothing &\text{otherwise}\end{cases}
$$
A term $t$ is said to be *closed* if $t$ does not have any variables, that is $\freevars(t)=\varnothing$.



Terms don't only represent expression, they can represent statements.

#### Substitutions

Substitutions are objects that carry information about replacing variables with terms. Substitutions can be represented in several different ways (we will pick one later), but we can describe them generally by certain rules they must follow. 

The set of substitutions is denoted by $\tsf{Subst}$. Along with the set $\tsf{Subst}$, we must have two operations,

- a method for applying a substitution $\sigma \in \tsf{Subst}$ to a term $t$, by replacing every occurance of a free variable of $t$ with its coresponding term mapped by $\sigma$, resulting in the term $t[\sigma]\in\tsf{Term}$,
- and a function $\odot : \tsf{Subst}\times \tsf{Subst} \to \tsf{Subst}$, which is way to compose any two substitutions $\sigma,\tau\in\tsf{Subst}$, resulting in a new substitution $\sigma\odot\tau=\tau\sigma\in\tsf{Subst}$ by applying $\tau$ first, then $\sigma$. (it is convention to write $\tau\sigma$ to mean $\sigma \odot \tau$)

Composing substitutions must obey certain rules, that is, for all $t\in\tsf{Term}$ and $\sigma,\tau\in\tsf{Subst}$,
$$
(t[\tau])[\sigma]=t[\tau\sigma]=t[\sigma\odot\tau],
$$
meaning that applying $\tau$ to $t$, then applying $\sigma$ is the same as applying $\tau\sigma=\sigma\tau$ to $t$. Substitutions must also distribute through constructed terms, that is, 
$$
f(t_1,\ldots,t_n)[\sigma]=f(t_1[\sigma],\ldots,t_n[\sigma]).
$$



Dictionaries are one such instance for substitutions. As you may know, a dictionary is a collection of pairs of keys and values, with the condition that each key may only map to a single value. 

Given a sets $K$ of keys and $V$ of values, a dictionary $d$ with keys in $K$ and values in $V$ is a relation on $K$ and $V$, that is, $d\subseteq K\times V$ or equivalently $d\in \mathcal{P}(K\times V)$. But $\mathcal{P}(K\times V)$ isn't the set of all dictionaries, as there are some relations on $K$ and $V$ that don't map the same key to just one value (such as $d=K\times V\in \mathcal{P}(K\times V)$). So in order to be very specific and narrow down the set of dictionaries, we define the set $\dict K V \subseteq \mathcal{P}(K\times V)$ to be that such set,
$$
\dict K V = \lbrace d \subseteq K \times V \ | \ d : \dom(K) \to V\rbrace,
$$
where $\dom(d)=\lbrace k \ | \ (k,v) \in d\rbrace$ is the set of keys for the dictionary $d$. This means that every $d\in \dict K V$ is a function on its set of keys. 

With that out of the way, for our case, the keys are variables and the values are terms, so $\tsf{Subst} = \dict {\tsf{Var}} {\tsf{Term}}$. The operation for applying a substitution (dictionary) $\sigma \in \tsf{Subst}$ to a term $t\in \tsf{Term}$, is defined in a pretty straightforward way,
$$
t[\sigma]\eqdef \begin{cases}\sigma(x) & \text{if $t=x\in \tsf{Var}$ and $x\in\dom(\sigma)$}\\\ f(t_1[\sigma],\ldots,t_n[\sigma]) & \text{if $t=f(t_1,\ldots,t_n)$}\\\ t & \text{otherwise}\end{cases}
$$
which means every variable $x\in\freevars(t)$ will be replaced by the corresponding term $\sigma(x)$, provided $x$ is a key in $\sigma$, that is $x\in\dom(\sigma)$. Otherwise, $x$ will remain unchanged in $t$ if $x\not\in\dom(\sigma)$. 

As an example, consider the substitution $\sigma=\lbrace (b,\times(2,c))\rbrace\in\dict {\tsf{Var}}{\tsf{Term}}=\tsf{Subst}$, and then applying $\sigma$ to the term $+(a,b)$​,
$$
+(a,b)[\sigma]=+(a,b)[\lbrace (b,\times(2,c))\rbrace]=+(a[\lbrace (b,\times(2,c))\rbrace],b[\lbrace (b,\times(2,c))\rbrace])=+(a,\times(2,c)).
$$






Unification in general is a nondeterministic process. Two terms $t_1$ and $t_2$ can be unified in many different ways, as in, there may be more than one unifier $\sigma$ such that $\sigma$ unifies $t_1$ and $t_2$, that is $t_1[\sigma]=t_2[\sigma]$. Thankfully, there is a deterministic process to find a unifier with minimal effort. 





